{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T·∫£i d·ªØ li·ªáu v√† th·ª±c hi·ªán training m√¥ h√¨nh h·ªèi ƒë√°p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "KLqeqWryHQ3Q",
        "outputId": "5efb1755-0c74-4032-e040-49fa097a677b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8464' max='16425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8464/16425 1:24:12 < 1:19:13, 1.67 it/s, Epoch 1.55/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.198700</td>\n",
              "      <td>1.150598</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16425' max='16425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16425/16425 2:45:20, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.198700</td>\n",
              "      <td>1.150598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>1.102068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.729500</td>\n",
              "      <td>1.138339</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('./qa_model/tokenizer_config.json',\n",
              " './qa_model/special_tokens_map.json',\n",
              " './qa_model/vocab.txt',\n",
              " './qa_model/added_tokens.json',\n",
              " './qa_model/tokenizer.json')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# T·∫£i model v√† tokenizer\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "# T·∫£i d·ªØ li·ªáu\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        context_start = sequence_ids.index(1)\n",
        "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
        "\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            start_positions.append(context_start + sum([1 for o in offset[context_start:context_end] if o[1] < start_char]))\n",
        "            end_positions.append(context_start + sum([1 for o in offset[context_start:context_end] if o[1] < end_char]))\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./qa_model\")\n",
        "tokenizer.save_pretrained(\"./qa_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T·∫£i m√¥ h√¨nh ƒë√£ l∆∞u ƒë·ªÉ s·ª≠ d·ª•ng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6kSs9NMIwFk"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
        "import torch\n",
        "\n",
        "# T·∫£i m√¥ h√¨nh v√† tokenizer ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"./qa_model\")\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"./qa_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km01iSoxwR0G"
      },
      "outputs": [],
      "source": [
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=384)\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNqLzcJ7wee5",
        "outputId": "5bdcd4ec-e5df-4b48-8bc7-bbedf97b08c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: painting, music, singing, and traveling\n"
          ]
        }
      ],
      "source": [
        "context = \"\"\"\n",
        "I am an artist, I like painting, music, singing, and traveling\n",
        "\"\"\"\n",
        "question = \"Who likes to draw?\"\n",
        "\n",
        "answer = answer_question(question, context)\n",
        "print(f\"Answer: {answer}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
